{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Вычисляет оценку **BLEU-2** для сгенерированной последовательности.\n",
        "Использует сглаживание для предотвращения нулевой оценки при отсутствии совпадений.\n",
        "\n",
        "**Метрика:** BLEU (Bilingual Evaluation Understudy)\n",
        "**Назначение:** Оценивает сходство сгенерированного текста с эталонным. Для символьного уровня сравнивает совпадение пар (2-грамм) символов, а не слов, что менее информативно, чем для слов.\n",
        "\n",
        "**Weights=(0.5, 0.5):** Означает, что используются отдельные символы и пары символов с равным весом.\n",
        "\n",
        "---\n",
        "\n",
        "**clean_text:** Предварительная обработка текста: приведение к нижнему регистру,удаление всех символов, кроме кириллицы, латиницы, пробелов и основных знаков препинания. Очистка необходима для формирования чистого словаря символов (VOCAB)."
      ],
      "metadata": {
        "id": "vePA9SZ19JyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEQUENCE_LENGTH  Максимальная длина последовательности символов\n",
        "\n",
        "EMBEDDING_DIM    Размерность векторного представления символа\n",
        "\n",
        "MODEL_DIM        Размерность модели\n",
        "\n",
        "NUM_HEADS        Количество \"голов\" в Multi-Head Attention\n",
        "\n",
        "NUM_LAYERS       Количество слоев декодера\n",
        "\n",
        "FF_DIM           Размерность скрытого слоя Feed-Forward\n",
        "\n",
        "LEARNING_RATE    Скорость обучения\n",
        "\n",
        "NUM_EPOCHS       Количество эпох\n",
        "\n",
        "BATCH_SIZE       Вводим батчи для Transformer\n",
        "\n",
        "TEST_FRAGMENT_LENGTH Длина сгенерированного фрагмента\n",
        "\n",
        "START_TOKEN      Токен начала последовательности\n",
        "\n",
        "PAD_TOKEN        Токен заполнения"
      ],
      "metadata": {
        "id": "9VQHbOJ8-Qg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import re\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import collections\n",
        "\n",
        "# Вспомогательные функции\n",
        "def calculate_bleu2(reference_words, generated_words):\n",
        "    if len(generated_words) < 2:\n",
        "        return 0.0\n",
        "    chencherry = SmoothingFunction()\n",
        "    # Сравнение биграмм и униграмм символов\n",
        "    score = sentence_bleu([reference_words], generated_words, weights=(0.5, 0.5), smoothing_function=chencherry.method1)\n",
        "    return score\n",
        "\n",
        "# Функция для очистки текста\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    # Удаляем все, кроме кириллицы, латиницы, пробелов и знаков препинания для символьного уровня\n",
        "    text = re.sub(r'[^а-яa-zё\\s.,!?-]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Гиперпараметры\n",
        "SEQUENCE_LENGTH = 30\n",
        "EMBEDDING_DIM = 64\n",
        "MODEL_DIM = 128\n",
        "NUM_HEADS = 4\n",
        "NUM_LAYERS = 4\n",
        "FF_DIM = 512\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "TEST_FRAGMENT_LENGTH = 100\n",
        "START_TOKEN = '<START>'\n",
        "PAD_TOKEN = '<PAD>'"
      ],
      "metadata": {
        "id": "4dmSIzL4ru4N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка текста\n",
        "# Используем \"Мертвые души\"\n",
        "FILE_NAME = \"Гоголь Николай. Мертвые души.txt\"\n",
        "corpus_text = \"\"\n",
        "\n",
        "try:\n",
        "    with open(FILE_NAME, 'r', encoding='windows-1251') as f:\n",
        "        corpus_text = f.read()\n",
        "    print(f\"Текст успешно загружен из файла: {FILE_NAME}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Ошибка: Файл '{FILE_NAME}' не найден. Проверьте имя файла.\")\n",
        "except UnicodeDecodeError as e:\n",
        "    print(f\"Критическая ошибка кодировки: Не удалось прочитать файл. Подробности: {e}\")\n",
        "\n",
        "# Подготовка данных на уровне символов\n",
        "if not corpus_text:\n",
        "    print(\"Невозможно продолжить: Корпус текста пуст.\")\n",
        "else:\n",
        "    # Очистка и токенизация\n",
        "    cleaned_text = clean_text(corpus_text)\n",
        "    # Создание словаря символов\n",
        "    all_chars = sorted(list(set(cleaned_text)))\n",
        "    # Добавление специальных токенов\n",
        "    VOCAB = [PAD_TOKEN, START_TOKEN] + all_chars\n",
        "\n",
        "    char_to_ix = {char: i for i, char in enumerate(VOCAB)}\n",
        "    ix_to_char = {i: char for char, i in char_to_ix.items()}\n",
        "    VOCAB_SIZE = len(VOCAB)\n",
        "    # Преобразование всего текста в индексы\n",
        "    text_indices = [char_to_ix[c] for c in cleaned_text]\n",
        "\n",
        "    print(f\"\\nРазмер словаря (VOCAB_SIZE): {VOCAB_SIZE} символов\")\n",
        "    print(f\"Общее количество символов в корпусе: {len(text_indices)}\")\n",
        "\n",
        "    # Формирование обучающих последовательностей\n",
        "    data = []\n",
        "    # Каждая последовательность имеет длину SEQUENCE_LENGTH\n",
        "    for i in range(len(text_indices) - SEQUENCE_LENGTH):\n",
        "        # Input - символы до T-1 для предсказания\n",
        "        input_sequence = text_indices[i : i + SEQUENCE_LENGTH]\n",
        "        # Target - символы с 1 до T правильные следущие символы\n",
        "        target_sequence = text_indices[i + 1 : i + SEQUENCE_LENGTH + 1]\n",
        "        data.append((input_sequence, target_sequence))\n",
        "\n",
        "    print(f\"Количество обучающих последовательностей: {len(data)}\")\n",
        "\n",
        "    # Функция для преобразования последовательности индексов в тензор PyTorch\n",
        "    def make_sequence_tensor(sequence_indices):\n",
        "        return torch.tensor(sequence_indices, dtype=torch.long)"
      ],
      "metadata": {
        "id": "2pXtBQgCrPva",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac09617-30e8-4913-8e8d-24c101ec0d70"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Текст успешно загружен из файла: Гоголь Николай. Мертвые души.txt\n",
            "\n",
            "Размер словаря (VOCAB_SIZE): 42 символов\n",
            "Общее количество символов в корпусе: 64872\n",
            "Количество обучающих последовательностей: 64842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**subsequent_mask**: Создание маски заглядывания вперед. Критически важна для декодера: она гарантирует, что при предсказании символа на позиции 'i' модель видит только символы до позиции 'i-1'. Это имитирует однонаправленную природу RNN и предотвращает \"жульничество\" модели, когда она видит то, что должна предсказать.\n",
        "\n",
        "---\n",
        "\n",
        "**MultiHeadAttention**: Механизм многоголового внимания. Позволяет модели одновременно обращать внимание на разные части входной последовательности используя разные head\n",
        "\n",
        "---\n",
        "\n",
        "**Feed-Forward Layer**: Полносвязный (Feed-Forward) слой. Применяется независимо к каждой позиции в последовательности. Состоит из двух линейных преобразований с ReLU и Dropout.\n",
        "\n",
        "---\n",
        "\n",
        "**Decoder Layer**:\n",
        "Один слой декодера Transformer. Состоит из:\n",
        "1. Masked Multi-Head Self-Attention внимание только на предыдущие символы\n",
        "2. Add & Norm добавление остаточного соединения и нормализация слоя\n",
        "3. Positionwise Feed Forward\n",
        "4. Add & Norm добавление остаточного соединения и нормализация слоя\n",
        "\n",
        "---\n",
        "\n",
        "**TransformerDecoder**: Основная модель. Собирает все компоненты для символьного языкового моделирования."
      ],
      "metadata": {
        "id": "GqyqWdaFAtXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ключевые элементы Transformer decoder\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        # Вычисление коэффициентов для синусов/косинусов\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Применение синуса к четным индексам\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Применение косинуса к нечетным индексам\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        # Сохранение как буфер, не обновляется во время обучения\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Добавляем позиционное кодирование к эмбеддингам\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# Multi-Head Attention и Маска\n",
        "def subsequent_mask(size):\n",
        "    # Маска, чтобы предотвратить внимание к последующим позициям\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1).type(torch.uint8)\n",
        "    return mask == 0\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Линейные преобразования для Q (Query), K (Key), V (Value)\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        # Финальное преобразование после объединения голов\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        N = q.size(0)\n",
        "\n",
        "        # Применяем линейные преобразования и разделяем на головы по последнему измерению)\n",
        "        q = self.query(q).view(N, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.key(k).view(N, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.value(v).view(N, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        # q, k, v теперь: (N, num_heads, seq_len, d_k)\n",
        "\n",
        "        # Scaled Dot-Product Attention (Q*K^T / sqrt(d_k))\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            # Применяем маску заменяем запрещенные (masked) значения на очень маленькое число (-1e9)\n",
        "            # Это гарантирует, что softmax даст им нулевую вероятность.\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        # Преобразование в веса внимания (вероятности)\n",
        "        p_attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Умножение на V и объединение голов\n",
        "        x = torch.matmul(p_attn, v)\n",
        "        x = x.transpose(1, 2).contiguous().view(N, -1, self.num_heads * self.d_k)\n",
        "\n",
        "        # Финальное линейное преобразование\n",
        "        return self.out(x)\n",
        "\n",
        "# Feed-Forward Layer\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "# Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Masked Multi-Head Self-Attention\n",
        "        attn_output = self.self_attn(x, x, x, mask=mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "\n",
        "        # Feed Forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "        return x\n",
        "\n",
        "# Сборка Transformer Decoder\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        # Преобразование индекса символа в вектор d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff)\n",
        "            # Повторение Decoder Layer N раз\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: (batch_size, seq_len) последовательность индексов символов\n",
        "\n",
        "        # Embedding\n",
        "        x = self.embedding(src)\n",
        "\n",
        "        # Positional Encoding\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Создание маски для Self-Attention\n",
        "        seq_len = x.size(1)\n",
        "        mask = subsequent_mask(seq_len).unsqueeze(0).to(x.device)\n",
        "\n",
        "        # Проход через слои декодера\n",
        "        for layer in self.layers:\n",
        "            # Маска для Multi-Head Attention\n",
        "            x = layer(x, mask.unsqueeze(1))\n",
        "\n",
        "        # Финальный линейный слой для прогнозирования символа\n",
        "        output = self.final_linear(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "4otUqO8tvDE4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train_model**: Итеративный процесс обучения модели на батчах данных с использованием DataLoader"
      ],
      "metadata": {
        "id": "8tzRQr_DC6eG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Метрика: Cross-Entropy Loss**\n",
        "\n",
        "**Назначение**: Основная функция потерь для задач классификации, включая языковое моделирование. Измеряет, насколько предсказанное распределение вероятностей отличается от истинного распределения (one-hot кодирование целевого символа).\n",
        "**ignore_index**: Игнорирует PAD_TOKEN при вычислении потерь, чтобы он не влиял на обучение.\n",
        "\n",
        "---\n",
        "**Оптимизатор:** Adam\n",
        "\n",
        "**Назначение:** Алгоритм, который регулирует скорость обучения (Learning Rate) для каждого параметра модели индивидуально, обеспечивая быструю сходимость.\n"
      ],
      "metadata": {
        "id": "wGwzWigwDmVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция обучения\n",
        "\n",
        "def train_model(model, loss_function, optimizer, data, num_epochs, batch_size):\n",
        "    print(\"--- Начинается обучение модели ---\")\n",
        "\n",
        "    # Преобразование данных в DataLoader для батчей\n",
        "    dataset = torch.utils.data.TensorDataset(\n",
        "        make_sequence_tensor([d[0] for d in data]), # Входы (X)\n",
        "        make_sequence_tensor([d[1] for d in data])  # Цели (Y)\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Определение устройства (GPU/CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        # Установка режима обучения\n",
        "        model.train()\n",
        "\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            # Обнуление градиентов\n",
        "            model.zero_grad()\n",
        "            # Прямой проход\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Loss: сравниваем outputs с targets\n",
        "            loss = loss_function(outputs.transpose(1, 2), targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            # Обратный проход (вычисление градиентов)\n",
        "            loss.backward()\n",
        "            # Обновление весов\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
        "            avg_loss = total_loss / len(dataloader)\n",
        "            print(f\"Эпоха {epoch+1}/{num_epochs}, Средняя Потеря (Loss): {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"--- Обучение завершено ---\")\n",
        "\n",
        "# Инициализация и Обучение\n",
        "if corpus_text:\n",
        "    model = TransformerDecoder(VOCAB_SIZE, MODEL_DIM, NUM_HEADS, FF_DIM, NUM_LAYERS)\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=char_to_ix[PAD_TOKEN])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Запускаем обучение\n",
        "    train_model(model, loss_function, optimizer, data, NUM_EPOCHS, BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz-cuQ1RvFes",
        "outputId": "c4297d93-0ec9-4941-812e-8bc3ff8737a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Начинается обучение модели ---\n",
            "Эпоха 5/100, Средняя Потеря (Loss): 0.9446\n",
            "Эпоха 10/100, Средняя Потеря (Loss): 0.7470\n",
            "Эпоха 15/100, Средняя Потеря (Loss): 0.6794\n",
            "Эпоха 20/100, Средняя Потеря (Loss): 0.6419\n",
            "Эпоха 25/100, Средняя Потеря (Loss): 0.6176\n",
            "Эпоха 30/100, Средняя Потеря (Loss): 0.6000\n",
            "Эпоха 35/100, Средняя Потеря (Loss): 0.5848\n",
            "Эпоха 40/100, Средняя Потеря (Loss): 0.5743\n",
            "Эпоха 45/100, Средняя Потеря (Loss): 0.5646\n",
            "Эпоха 50/100, Средняя Потеря (Loss): 0.5569\n",
            "Эпоха 55/100, Средняя Потеря (Loss): 0.5499\n",
            "Эпоха 60/100, Средняя Потеря (Loss): 0.5437\n",
            "Эпоха 65/100, Средняя Потеря (Loss): 0.5378\n",
            "Эпоха 70/100, Средняя Потеря (Loss): 0.5329\n",
            "Эпоха 75/100, Средняя Потеря (Loss): 0.5290\n",
            "Эпоха 80/100, Средняя Потеря (Loss): 0.5241\n",
            "Эпоха 85/100, Средняя Потеря (Loss): 0.5213\n",
            "Эпоха 90/100, Средняя Потеря (Loss): 0.5176\n",
            "Эпоха 95/100, Средняя Потеря (Loss): 0.5148\n",
            "Эпоха 100/100, Средняя Потеря (Loss): 0.5128\n",
            "--- Обучение завершено ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**generate_text_char**\n",
        "\n",
        "Функция для последовательной генерации символов.\n",
        "На каждом шаге модель предсказывает следующий символ, используя всю\n",
        "ранее сгенерированную последовательность.\n",
        "\n",
        "---\n",
        "\n",
        "**Температура (T):**\n",
        "\n",
        "**Высокое T (T>1)** делает распределение более плоским (более случайная генерация),\n",
        "\n",
        "**Низкое T (T<1)** делает распределение более резким (более детерминированная генерация)."
      ],
      "metadata": {
        "id": "AuGGZXA3EHuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция генерации текста\n",
        "def generate_text_char(model, start_char, length, temperature=0.5):\n",
        "    model.eval() # Режим оценки\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Инициализация с <START> токеном. Модель учится начинать последовательности с <START>\n",
        "    input_list = [char_to_ix[START_TOKEN], char_to_ix[start_char]]\n",
        "\n",
        "    # Отключение вычисления градиентов для ускорения и экономии памяти\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            # Подготовка входной последовательности\n",
        "            if len(input_list) > SEQUENCE_LENGTH:\n",
        "                 # Обрезаем до SEQUENCE_LENGTH если длина превышена\n",
        "                current_input = input_list[-SEQUENCE_LENGTH:]\n",
        "            else:\n",
        "                current_input = input_list\n",
        "\n",
        "            inputs = make_sequence_tensor(current_input).unsqueeze(0).to(device)\n",
        "\n",
        "            # Проход через модель\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Берем прогнозы только для последнего символа в последовательности (это то, что модель предсказывает следующим после current_seq_len-й позиции)\n",
        "            next_char_logits = outputs[0, -1, :]\n",
        "\n",
        "            # Контроль случайности через температуру\n",
        "            outputs_temp = next_char_logits / temperature\n",
        "            # Преобразование логитов в вероятности\n",
        "            probabilities = F.softmax(outputs_temp, dim=-1)\n",
        "\n",
        "            # Выбор следующего символа используется Multinomial Sampling выборка по вероятностям\n",
        "            next_char_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
        "\n",
        "            if next_char_idx == char_to_ix[PAD_TOKEN] or next_char_idx == char_to_ix[START_TOKEN]:\n",
        "                # Остановить генерацию при встрече спец. токена\n",
        "                break\n",
        "\n",
        "            input_list.append(next_char_idx)\n",
        "\n",
        "    # Преобразование индексов обратно в символы (исключая <START> токен)\n",
        "    generated_text_indices = input_list[1:]\n",
        "    generated_text = \"\".join([ix_to_char[i] for i in generated_text_indices])\n",
        "\n",
        "    # Возврат в режим обучения\n",
        "    model.train()\n",
        "    return generated_text\n",
        "\n",
        "# Запуск Генерации\n",
        "if corpus_text:\n",
        "    # START_CHAR = 'п' стартовый символ П\n",
        "    START_CHAR = 'п'\n",
        "    # Генерируем 100 символов\n",
        "    GENERATION_LENGTH = 100\n",
        "    TEMPERATURE = 0.7\n",
        "\n",
        "    print(f\"\\n--- Генерация текста (длина {GENERATION_LENGTH} символов) ---\")\n",
        "    print(f\"Стартовый символ: {START_CHAR}\")\n",
        "    print(f\"Температура: {TEMPERATURE}\")\n",
        "\n",
        "    generated_text_output = generate_text_char(model, START_CHAR, GENERATION_LENGTH, TEMPERATURE)\n",
        "    print(f\"Сгенерированный текст:\\n'{generated_text_output}'\")\n",
        "\n",
        "    # Оценка модели\n",
        "    if len(cleaned_text) > TEST_FRAGMENT_LENGTH:\n",
        "        reference_text = cleaned_text[len(cleaned_text)//2 : len(cleaned_text)//2 + TEST_FRAGMENT_LENGTH]\n",
        "\n",
        "        generated_chars = list(generated_text_output)\n",
        "        reference_chars = list(reference_text)\n",
        "\n",
        "        # Метрики BLEU-2 на символах\n",
        "        bleu_score = calculate_bleu2(reference_chars, generated_chars)\n",
        "\n",
        "        print(\"\\n--- Упрощенная количественная оценка (BLEU-2 на символах) ---\")\n",
        "        print(f\"Сравниваемый фрагмент (Test, {TEST_FRAGMENT_LENGTH} символов):\")\n",
        "        print(f\"'{reference_text}'\")\n",
        "        print(f\"Оценка BLEU-2 (на символах): {bleu_score:.4f}\")\n",
        "    else:\n",
        "        print(\"\\n--- Упрощенная количественная оценка ---\")\n",
        "        print(\"Невозможно рассчитать BLEU: тестовый корпус слишком мал или пуст.\")"
      ],
      "metadata": {
        "id": "NuvhtZPfxJZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3003eb18-f1a4-4fb3-be61-a4da7026c839"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Генерация текста (длина 100 символов) ---\n",
            "Стартовый символ: п\n",
            "Температура: 0.7\n",
            "Сгенерированный текст:\n",
            "'приления, сказал один мудрец. и знаете, павел иванович, нет, вы гость, говорил манилов, именно, очень'\n",
            "\n",
            "--- Упрощенная количественная оценка (BLEU-2 на символах) ---\n",
            "Сравниваемый фрагмент (Test, 100 символов):\n",
            "'кие вершины. под двумя из них видна была беседка с плоским зеленым куполом, деревянными голубыми кол'\n",
            "Оценка BLEU-2 (на символах): 0.4448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BLEU (Bilingual Evaluation Understudy)** — это одна из самых старых и наиболее распространенных метрик для количественной оценки качества машинного перевода, а также для оценки систем генерации текста (например, суммаризации или, в вашем случае, языкового моделирования).\n",
        "\n",
        "Ее главная **цель** — измерить степень сходства между сгенерированным (предсказанным) текстом и одним или несколькими эталонными (референсными) текстами."
      ],
      "metadata": {
        "id": "uhFIrUV3GFbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценка $0.4448$ означает что:\n",
        "1. Оценка далека от $0.0$, что говорит о том, что модель успешно освоила алфавит, общие орфографические паттерны, знаки препинания и, вероятно, структуру коротких часто встречающихся \"слов\"\n",
        "2. Для генерации осмысленного и беглого текста на уровне символов, где каждая ошибка в букве может разорвать биграмму, оценка $0.44$ указывает на то, что модель действительно научилась правильно писать слова, а не просто рандомно их генерировать.\n",
        "3. Высокая оценка BLEU-2 не гарантирует, что сгенерированный текст является осмысленным или грамматически правильным на уровне предложений. Модель могла просто хорошо имитировать последовательности букв."
      ],
      "metadata": {
        "id": "unpxE176KAFr"
      }
    }
  ]
}